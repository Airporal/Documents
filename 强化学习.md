# 强化学习笔记

> 记录强化学习相关算法的学习过程中的问题和解决方法、源码和实战。

## 1. 理论

### 1.1 概念剖析

+ 机器学习：代表一系列的从经验（数据）中学习的**技术。**

+ 监督学习：代表经验中包括标签的一类机器学习**问题**。
  + 回归问题：通过学习数据集中输入输出的映射关系，回答有多少的问题。
  + 分类问题：回答是什么的问题。
  + 搜索问题：根据与输入的相关性输出搜索结果。
  + 标注问题：为输入提供一系列的标注。
  + 推荐系统：根据用户的特征进行“个性化”推荐。
  + 序列学习：对连续的序列进行输出。如语音识别、机器翻译等。
  
+ 无监督学习：代表经验中没有标签的一类机器学习**问题。**
  + 聚类问题
  + 主成分分析
  + 因果分析
  + 生成对抗网络
  
+ 离线学习：代表先获取大量数据，然后在断开与环境的交互后进行学习的**一类算法。**

+ 在线学习：需要与环境交互中学习的一类算法。

+ 强化学习：通过智能体与环境的交互，在observation-action-reward中学习运动策略来解决机器学习问题的一类技术。
  + 马尔科夫决策过程：指环境可被完全观测的强化学习问题。
  + 上下文老虎机：指状态不依赖于之前的动作的强化学习问题。
  + 多臂老虎机：指没有状态，只有一组最初未知奖励的可用动作时的问题。
  
+ 深度学习：通过神经网络与梯度下降算法来解决机器学习问题的一类技术。当与强化学习结合时，称为深度强化学习。

+ 向量化：将单独的样本组成张量，使用张量运算库同时进行运算而不是使用循环进行计算。

+ 熵：信息论中用于量化数据中的信息内容的数值。单位是纳特，与比特类似，1纳特等于1.44比特。

+ 信息量：香农提出来用信息量来量化对不能完全预测的事件的“惊讶”程度。当赋予事件概率比较低时，惊讶程度会较大，信息量也较大。熵是信息论的期望。
  $$
  E(j)=-log(P(j))
  $$
  即：事件E表示让知道结果的人如此惊讶的量，事件发生的概率为P(j),则期望为：
  $$
  H(P)=\sum_j-P(j)log(P(j))
  $$

+ 交叉熵：可表示为主观概率为Q的观察者在看到根据概率P生成的数据时的预期惊讶。交叉熵的目标是：最大化观测数据的似然；最小化传达标签所需的惊讶。（让参数更对，模型输出与真实结果差距更低）

### 1.2 数学知识

### 1.3 Pytorch框架





---



### 1.4 模型优化方法

#### 1.4.1 注意力机制

#### 1.4.2 梯度下降优化

#### 1.4.3 学习率优化

## 2. 深度学习算法

### 2.1 线性回归

### 2.2 多层感知机

### 2.3 卷积神经网络

### 2.4 循环神经网络

## 3. 深度强化学习基础

### 3.0 马尔可夫决策过程

### 3.1 动态规划算法

### 3.2 时序差分

### 3.3 Dyna-Q

## 4. 深度强化学习算法

### 4.1 PPO

### 4.2 AC

### 4.3 SAC

### 4.3 DQN

### 4.4 TRPO

### 4.5 DDPG

## 5. 前沿

### 5.1 模仿学习

### 5.2 With MPC

### 5.3 Off-policy

### 5.4 GoRL

### 5.5 Multi-Agent

### 5.6 LLM 生成式奖励函数